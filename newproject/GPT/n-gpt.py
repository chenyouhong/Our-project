# -*- coding: utf-8 -*-
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import glob
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from dataclasses import dataclass


# ==================== 1. æ•°æ®é›†å®šä¹‰ ====================
class ALFADataset(Dataset):
    def __init__(self, file_paths, mode='train', scaler=None, L=50, H=20):
        self.L = L
        self.H = H
        # â˜…â˜…â˜… ä¿®æ”¹å (å¢åŠ  orientation.x, y, z, w)
        self.features = [
            'orientation.x', 'orientation.y', 'orientation.z', 'orientation.w',
            'linear_acceleration.x', 'linear_acceleration.y', 'linear_acceleration.z',
            'angular_velocity.x', 'angular_velocity.y', 'angular_velocity.z'
        ]

        raw_data_list = []
        for file in file_paths:
            df = pd.read_csv(file)
            # ç®€å•æ£€æŸ¥åˆ—åæ˜¯å¦å­˜åœ¨ï¼Œå®é™…éœ€æ ¹æ® CSV è¡¨å¤´è°ƒæ•´
            if not set(self.features).issubset(df.columns):
                continue
            data_chunk = df[self.features].values.astype(np.float32)
            raw_data_list.append(data_chunk)

        if not raw_data_list:
            raise ValueError("No valid data found in file paths.")

        full_data = np.concatenate(raw_data_list, axis=0)

        if mode == 'train':
            self.scaler = StandardScaler()
            self.normalized_data = self.scaler.fit_transform(full_data)
        else:
            assert scaler is not None, "Test mode requires a fitted scaler!"
            self.scaler = scaler
            self.normalized_data = self.scaler.transform(full_data)

        self.data = torch.tensor(self.normalized_data, dtype=torch.float32)
        self.valid_indices = len(self.data) - (self.L + self.H)

    def __len__(self):
        return max(0, self.valid_indices)

    def __getitem__(self, idx):
        hist_end = idx + self.L
        future_end = hist_end + self.H

        C_seq = self.data[idx: hist_end]  # Condition [L, D]
        Y_seq = self.data[hist_end: future_end]  # Target [H, D]
        return C_seq, Y_seq

    def get_scaler(self):
        return self.scaler


# ==================== 2. åŸºç¡€ç»„ä»¶ ====================
class SinusoidalPositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        L = x.size(1)
        return x + self.pe[:, :L, :]


class TimeEmbedding(nn.Module):
    def __init__(self, num_steps: int, d_model: int):
        super().__init__()
        self.emb = nn.Embedding(num_steps, d_model)
        self.mlp = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.SiLU(),
            nn.Linear(d_model, d_model),
        )

    def forward(self, t: torch.Tensor) -> torch.Tensor:
        x = self.emb(t)
        return self.mlp(x)


@dataclass
class DiffusionConfig:
    num_steps: int = 1000
    beta_start: float = 1e-4
    beta_end: float = 0.02


class GaussianDiffusionSchedule(nn.Module):
    def __init__(self, cfg: DiffusionConfig):
        super().__init__()
        self.num_steps = cfg.num_steps
        betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.num_steps)
        alphas = 1.0 - betas
        alpha_bars = torch.cumprod(alphas, dim=0)
        alpha_bars_prev = torch.cat([torch.ones(1), alpha_bars[:-1]], dim=0)
        posterior_variance = betas * (1.0 - alpha_bars_prev) / (1.0 - alpha_bars)

        self.register_buffer('betas', betas)
        self.register_buffer('alphas', alphas)
        self.register_buffer('alpha_bars', alpha_bars)
        self.register_buffer('alpha_bars_prev', alpha_bars_prev)
        self.register_buffer('posterior_variance', posterior_variance)

    def q_sample(self, x0, t, noise):
        B = x0.size(0)
        alpha_bar_t = self.alpha_bars[t].view(B, 1, 1)
        return torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1.0 - alpha_bar_t) * noise

    def p_sample_step(self, x_t, t_scalar, eps_pred):
        device = x_t.device
        t = torch.tensor(t_scalar, device=device, dtype=torch.long)
        alpha_t = self.alphas[t]
        alpha_bar_t = self.alpha_bars[t]
        alpha_bar_prev = self.alpha_bars_prev[t]

        # é¢„æµ‹ x0
        x0_hat = (x_t - torch.sqrt(1.0 - alpha_bar_t) * eps_pred) / torch.sqrt(alpha_bar_t)

        # åéªŒå‡å€¼
        coef1 = torch.sqrt(alpha_bar_prev) * self.betas[t] / (1.0 - alpha_bar_t)
        coef2 = torch.sqrt(alpha_t) * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t)

        # å¹¿æ’­å¤„ç†
        while coef1.dim() < x_t.dim(): coef1 = coef1.unsqueeze(-1)
        while coef2.dim() < x_t.dim(): coef2 = coef2.unsqueeze(-1)

        mean = coef1 * x0_hat + coef2 * x_t

        if t_scalar == 0:
            return mean

        var = self.posterior_variance[t]
        while var.dim() < x_t.dim(): var = var.unsqueeze(-1)
        noise = torch.randn_like(x_t)
        return mean + torch.sqrt(var) * noise


# ==================== 3. æ ¸å¿ƒæ¨¡å‹æ¶æ„ (æ”¹é€ é‡ç‚¹) ====================

class CondEncoder(nn.Module):
    def __init__(self, d_in, d_model, num_layers=2, num_heads=4, dim_ff=256, max_len=512):
        super().__init__()
        self.input_proj = nn.Linear(d_in, d_model)
        self.pos_enc = SinusoidalPositionalEncoding(d_model, max_len=max_len)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_ff,
                                                   batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, cond_seq):
        # â˜…â˜…â˜… å…³é”®ç‚¹ï¼šç§»é™¤ mean poolingï¼Œä¿ç•™å®Œæ•´åºåˆ—ä¾› Attention ä½¿ç”¨
        x = self.input_proj(cond_seq)
        x = self.pos_enc(x)
        h = self.encoder(x)
        return h  # [B, L, d_model]


class FutureBackbone(nn.Module):
    def __init__(self, d_model, num_layers=4, num_heads=4, dim_ff=512, max_len=256):
        super().__init__()
        self.pos_enc = SinusoidalPositionalEncoding(d_model, max_len=max_len)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_ff,
                                                   batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, x):
        x = self.pos_enc(x)
        return self.encoder(x)


class UAVDiffusionModel(nn.Module):
    def __init__(self, D_cond, D_target, diffusion_cfg, d_model_cond=128, d_model_future=128):
        super().__init__()
        self.D_cond = D_cond
        self.D_target = D_target

        self.cond_encoder = CondEncoder(D_cond, d_model_cond)
        self.time_embed = TimeEmbedding(diffusion_cfg.num_steps, d_model_future)

        # â˜…â˜…â˜… æ”¹é€ ï¼šè¾“å…¥ç»´åº¦ç¿»å€ (D_target * 2)ï¼Œæ¥æ”¶ [Y_t, self_cond]
        self.future_proj = nn.Linear(D_target * 2, d_model_future)

        # â˜…â˜…â˜… æ”¹é€ ï¼šCross-Attention æ¨¡å—
        self.cross_attn = nn.MultiheadAttention(embed_dim=d_model_future, num_heads=4, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model_future)

        self.future_backbone = FutureBackbone(d_model=d_model_future)
        self.out_proj = nn.Linear(d_model_future, D_target)

    def forward(self, C, Y_t, t, self_cond=None):
        # 1. ç¼–ç å†å² -> Key, Value [B, L, d]
        cond_seq = self.cond_encoder(C)

        # 2. å¤„ç† Self-Conditioning
        if self_cond is None:
            self_cond = torch.zeros_like(Y_t)

        # æ‹¼æ¥ Input: [B, H, D*2]
        x_in = torch.cat([Y_t, self_cond], dim=-1)

        # 3. Embedding + Time
        future_emb = self.future_proj(x_in)  # [B, H, d]
        t_emb = self.time_embed(t).unsqueeze(1)  # [B, 1, d]
        query = future_emb + t_emb  # [B, H, d]

        # 4. â˜…â˜…â˜… Cross-Attention: Future æŸ¥è¯¢ History
        attn_out, _ = self.cross_attn(query, cond_seq, cond_seq)
        x = self.norm1(query + attn_out)  # Residual + Norm

        # 5. Backbone & Output
        h = self.future_backbone(x)
        return self.out_proj(h)


# ==================== 4. è¾…åŠ©å‡½æ•° ====================

def predict_x0_from_xt(schedule, xt, eps_pred, t):
    """è¾…åŠ©ï¼šä» x_t åæ¨ x_0"""
    B = xt.size(0)
    alpha_bar_t = schedule.alpha_bars.to(xt.device)[t].view(B, 1, 1)
    sqrt_alpha_bar = torch.sqrt(alpha_bar_t)
    sqrt_one_minus = torch.sqrt(1.0 - alpha_bar_t)
    return (xt - sqrt_one_minus * eps_pred) / sqrt_alpha_bar


def training_step(model, schedule, C_batch, Y0_batch, optimizer, device):
    """è®­ç»ƒæ­¥ï¼šåŒ…å« Self-Conditioning éšæœºä¸¢å¼ƒ"""
    model.train()
    C_batch = C_batch.to(device)
    Y0_batch = Y0_batch.to(device)
    B = Y0_batch.size(0)

    t = torch.randint(0, schedule.num_steps, (B,), device=device).long()
    eps = torch.randn_like(Y0_batch)
    Y_t = schedule.q_sample(Y0_batch, t, eps)

    # â˜…â˜…â˜… Self-Cond è®­ç»ƒç­–ç•¥: 50% æ¦‚ç‡ä½¿ç”¨çœŸå®å€¼æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
    if torch.rand(1) < 0.5:
        with torch.no_grad():
            self_cond = Y0_batch  # æ¨¡æ‹Ÿâ€œå®Œç¾é¢„æµ‹â€
    else:
        self_cond = torch.zeros_like(Y0_batch)

    eps_pred = model(C_batch, Y_t, t, self_cond)
    loss = F.mse_loss(eps_pred, eps)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()


@torch.no_grad()
def impute_future_trajectory(model, schedule, C, Y_obs, mask, device):
    """æ ¸å¿ƒæ¨ç†ï¼šå¸¦ Mask çš„è¡¥å…¨é€»è¾‘"""
    model.eval()
    B, H, D = Y_obs.shape
    Y_t = torch.randn_like(Y_obs)
    self_cond = torch.zeros_like(Y_obs)  # åˆå§‹ self_cond

    for t_scalar in reversed(range(schedule.num_steps)):
        t = torch.full((B,), t_scalar, device=device).long()

        # é¢„æµ‹å™ªå£°
        eps_pred = model(C, Y_t, t, self_cond)

        # æ›´æ–° self_cond (ç”¨ x0 çš„ä¼°è®¡å€¼)
        x0_pred = predict_x0_from_xt(schedule, Y_t, eps_pred, t)
        self_cond = x0_pred.detach()

        # åå‘ä¸€æ­¥
        Y_prev = schedule.p_sample_step(Y_t, t_scalar, eps_pred)

        # â˜…â˜…â˜… RePaint ç­–ç•¥: å·²çŸ¥éƒ¨åˆ†å¼ºåˆ¶æ›¿æ¢
        if t_scalar > 0:
            noise = torch.randn_like(Y_obs)
            t_prev = torch.full((B,), t_scalar - 1, device=device).long()
            Y_obs_t = schedule.q_sample(Y_obs, t_prev, noise)
        else:
            Y_obs_t = Y_obs

        Y_t = mask * Y_obs_t + (1 - mask) * Y_prev

    return Y_t


@torch.no_grad()
def compute_imputation_score(model, schedule, C, Y_true, device, mask_ratio=0.5):
    """è®¡ç®—è¡¥å…¨è¯¯å·®åˆ†æ•°"""
    model.eval()
    # Mask: 1=Keep, 0=Masked
    mask = (torch.rand_like(Y_true) > mask_ratio).float().to(device)

    Y_imputed = impute_future_trajectory(model, schedule, C, Y_true, mask, device)

    # ä»…è®¡ç®— Mask éƒ¨åˆ†çš„ MSE
    loss = F.mse_loss(Y_imputed * (1 - mask), Y_true * (1 - mask), reduction='none')
    score = loss.sum() / ((1 - mask).sum() + 1e-6)
    return score.item()


def determine_engineering_threshold(model, schedule, loader, device, mask_ratio=0.5, k=3.0):
    """åŸºäºéªŒè¯é›†è®¡ç®—é˜ˆå€¼"""
    print("\n[Threshold] Calibrating on normal data...")
    scores = []
    for i, (C, Y) in enumerate(loader):
        C, Y = C.to(device), Y.to(device)
        s = compute_imputation_score(model, schedule, C, Y, device, mask_ratio)
        scores.append(s)
        if i % 20 == 0: print(f" -> Batch {i}...")

    mu = np.mean(scores)
    std = np.std(scores)
    thresh = mu + k * std
    print(f"[Stats] Mean: {mu:.4f}, Std: {std:.4f} => Threshold: {thresh:.4f}")
    return thresh


# ==================== 5. ä¸»æµç¨‹ ====================

def main_alfa():
    device = torch.device('xpu' if torch.xpu.is_available() else 'cpu')
    print(f"Using device: {device}")

    # å‚æ•°é…ç½®
    L, H = 50, 20
    D_cond, D_target = 10, 10
    train_epochs = 30  # å»ºè®®å¢åŠ  epoch
    mask_ratio = 0.5  # é®æŒ¡ 50% è¿›è¡Œè¡¥å…¨æµ‹è¯•

    # æ–‡ä»¶è·¯å¾„ (è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹)
    train_files = glob.glob('data/alfa/train/**/mavros-imu-data.csv', recursive=True)
    test_files = glob.glob('data/alfa/test/**/mavros-imu-data.csv', recursive=True)

    if not train_files:
        print("Error: No train files found.")
        return

    # æ•°æ®é›†
    print("Loading Data...")
    train_dataset = ALFADataset(train_files, mode='train', L=L, H=H)
    scaler = train_dataset.get_scaler()
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)

    # åˆå§‹åŒ–æ¨¡å‹
    cfg = DiffusionConfig(num_steps=1000)
    schedule = GaussianDiffusionSchedule(cfg).to(device)
    model = UAVDiffusionModel(D_cond, D_target, cfg).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # â˜…â˜…â˜… å¼ºåˆ¶é‡æ–°è®­ç»ƒ (ä¸åŠ è½½æ—§æƒé‡)
    # print("\n>>> Phase 1: Training (New Architecture)...")
    # model.train()
    # for epoch in range(train_epochs):
    #     total_loss = 0
    #     for C, Y in train_loader:
    #         loss = training_step(model, schedule, C, Y, optimizer, device)
    #         total_loss += loss
    #     print(f"Epoch {epoch + 1}/{train_epochs}, Avg Loss: {total_loss / len(train_loader):.6f}")
    #
    # torch.save(model.state_dict(), "uav_imputation_model.pth")
    print("\n>>> Phase 1: Loading Pre-trained Model...")
    # åŠ è½½æ‚¨åˆšæ‰ç”Ÿæˆçš„ .pth æ–‡ä»¶
    if os.path.exists("uav_imputation_model.pth"):
        model.load_state_dict(torch.load("uav_imputation_model.pth"))
        print("Success: Loaded 'uav_imputation_model.pth'")
    else:
        print("Error: Model file not found!")
        return

    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    # è®¡ç®—é˜ˆå€¼
    calib_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)
    threshold = determine_engineering_threshold(model, schedule, calib_loader, device, mask_ratio=mask_ratio)

    # æµ‹è¯•ç¯èŠ‚
    print("\n>>> Phase 2: Testing...")
    if not test_files: return

    test_file = test_files[0]  # å–ä¸€ä¸ªæ–‡ä»¶æµ‹è¯•
    print(f"Testing on: {test_file}")
    test_dataset = ALFADataset([test_file], mode='test', scaler=scaler, L=L, H=H)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

    scores = []
    with torch.no_grad():
        for i, (C, Y) in enumerate(test_loader):
            C, Y = C.to(device), Y.to(device)
            s = compute_imputation_score(model, schedule, C, Y, device, mask_ratio=mask_ratio)
            scores.append(s)
            if i % 100 == 0: print(f"Step {i}...")

        # ... (å‰æ¥ Phase 2 æµ‹è¯•å¾ªç¯ï¼Œscores åˆ—è¡¨å·²è®¡ç®—å®Œæ¯•) ...
        # ==================== ä¼˜åŒ–æ­¥éª¤ï¼šå»å™ªä¸å››ç»´è¯„ä¼° ====================
        print("\n>>> Phase 3: Final Optimization & Evaluation...")

        # 1. æ»‘åŠ¨å¹³å‡å¹³æ»‘ (å»å™ªå…³é”®)
        # çª—å£è¶Šå¤§æ›²çº¿è¶Šå¹³æ»‘ï¼Œä½†å“åº”è¶Šæ…¢ï¼›å»ºè®®å– 5-10
        window_size = 5
        smoothed_scores = pd.Series(scores).rolling(window=window_size, min_periods=1).mean().values

        # 2. æ„é€  Ground Truth (çœŸå®æ ‡ç­¾)
        # æ³¨æ„ï¼šALFA æ•°æ®é›†é€šå¸¸åœ¨æ–‡ä»¶ä¸­é—´å‘ç”Ÿæ•…éšœï¼Œéœ€æ ¹æ®æ–‡ä»¶åæˆ– result.png çš„çªå˜ç‚¹æ‰‹åŠ¨æŒ‡å®š
        y_true = np.zeros(len(scores))

        # â˜…â˜…â˜… å…³é”®ï¼šè¯·æ ¹æ®ä¸Šä¸€æ­¥ result.png ä¸­æ³¢å½¢çªå˜çš„ä½ç½®ä¿®æ”¹æ­¤å€¼
        # ä¾‹å¦‚ï¼šå¦‚æœå›¾ä¸­ç¬¬ 100 æ­¥å¼€å§‹é£™å‡ï¼Œå°±è®¾ä¸º 100
        failure_start_index = 100

        if "failure" in test_file or "carbon" in test_file:
            y_true[failure_start_index:] = 1  # æ ‡è®°æ•…éšœåŒºé—´
            print(f"[Ground Truth] Failure marked starting at index {failure_start_index}")
        else:
            print("[Ground Truth] Normal flight assumed.")

        # 3. é¢„æµ‹ä¸æŒ‡æ ‡è®¡ç®—
        # ä½¿ç”¨å¹³æ»‘åçš„åˆ†æ•°ä¸ä¹‹å‰è®¡ç®—çš„é˜ˆå€¼å¯¹æ¯”
        y_pred = (smoothed_scores > threshold).astype(int)

        acc = accuracy_score(y_true, y_pred)
        prec = precision_score(y_true, y_pred, zero_division=0)
        rec = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)

        # 4. æ‰“å°æœ€ç»ˆæˆç»©å•
        print("\n" + "=" * 45)
        print(f"ğŸ† Final Performance Report (Window={window_size})")
        print("=" * 45)
        print(f"Accuracy  (å‡†ç¡®ç‡): {acc:.2%}")
        print(f"Precision (ç²¾ç¡®ç‡): {prec:.2%}")
        print(f"Recall    (å¬å›ç‡): {rec:.2%}  <-- é‡ç‚¹å…³æ³¨ï¼Œä¸èƒ½æ¼æŠ¥ï¼")
        print(f"F1 Score  (ç»¼åˆåˆ†): {f1:.4f}")
        print("=" * 45)

        # 5. å¯è§†åŒ–å¯¹æ¯”
        plt.figure(figsize=(12, 6))
        plt.plot(scores, color='lightgray', label='Raw Score (Noisy)')
        plt.plot(smoothed_scores, color='blue', linewidth=2, label=f'Smoothed Score (MA={window_size})')
        plt.axhline(threshold, color='red', linestyle='--', label='Threshold')

        # æ ‡è®°çœŸå®æ•…éšœåŒºåŸŸ
        if np.sum(y_true) > 0:
            plt.axvspan(failure_start_index, len(scores), color='red', alpha=0.1, label='Ground Truth Failure')

        plt.title(f"Final Detection: F1={f1:.3f} | Recall={rec:.3f}")
        plt.legend()
        plt.tight_layout()
        plt.savefig('final_result_optimized.png')
        plt.show()
        print("Done.")


if __name__ == "__main__":
    main_alfa()